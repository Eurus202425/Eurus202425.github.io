<hr>
<p>title: 机器学习笔记（1）—————— C4.5算法，Python实现(陆续更新)<br>date: 2019-10-27 13:28:54<br>tags:</p>
<hr>
<p>决策树类型算法主要有ID3、C4.5和CART算法。这篇博客主要研究C4.5算法的原理及实现过程。<em>(纯属小白、写的不完善 、陆续更新优化 : )，增加画图、剪枝等功能)</em></p>
<h1 id="1-原理："><a href="#1-原理：" class="headerlink" title="1.原理："></a>1.原理：</h1><h2 id="1）思想："><a href="#1）思想：" class="headerlink" title="1）思想："></a>1）思想：</h2><p>总体思路为：通过训练集计算属性的信息熵、信息增益和信息增益率来判定当前最优的划分属性（即信息增益最大的属性），通过选出的最优属性依次递归构造二叉树（通过字典存储）。再利用构造好的二叉树对测试数据进行分类。</p>
<h2 id="2）类别：监督学习"><a href="#2）类别：监督学习" class="headerlink" title="2）类别：监督学习"></a>2）类别：监督学习</h2><h2 id="3）优点："><a href="#3）优点：" class="headerlink" title="3）优点："></a>3）优点：</h2><p>3.1 能够处理连续型取值的属性<br>3.2 通过信息增益率而不是信息增益来选择最优划分属性，避免选择偏向分支更多的属性<br>3.3 可处理缺失值<br>3.4 可对树进行预剪枝，避免造成过拟合</p>
<h2 id="4）基本概念和数学公式："><a href="#4）基本概念和数学公式：" class="headerlink" title="4）基本概念和数学公式："></a>4）基本概念和数学公式：</h2><h3 id="4-1-信息熵"><a href="#4-1-信息熵" class="headerlink" title="4.1 信息熵"></a>4.1 信息熵</h3><p>“信息熵”是度量样本集合纯度最常用的一种指标。（个人理解是，信息熵代表数据的混乱程度。熵增，复杂度上升；熵减，复杂度下降，若一个分类使得信息熵减小，则混乱程度下降，说明有助于我们分类）公式如下：<br><img src="C4-5%E7%AE%97%E6%B3%95Python%E5%AE%9E%E7%8E%B0/%E4%BF%A1%E6%81%AF%E7%86%B5.png" alt="信息熵"></p>
<h3 id="4-2-信息增益"><a href="#4-2-信息增益" class="headerlink" title="4.2 信息增益"></a>4.2 信息增益</h3><p>“信息增益”是分类后的信息熵相对于原信息熵的减少量，也就是说信息增益越大，信息熵减小得越多，分类越有效。公式如下：<br><img src="C4-5%E7%AE%97%E6%B3%95Python%E5%AE%9E%E7%8E%B0/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A.png" alt="信息增益"></p>
<h3 id="4-3-增益率"><a href="#4-3-增益率" class="headerlink" title="4.3 增益率"></a>4.3 增益率</h3><p>原本信息增益就可以解决分类问题，那为什么要提出信息增益率之一概念呢？原因如下：信息增益有一个缺点，当一个属性的取值多而松散时*（比如 ID 这个属性，取值不重复且无规律，对分类并无贡献，但在计算信息增益时，信息增益会偏大，对于属性的选择会造成误导。但是若使用信息增益率，则能很好的解决这一问题。图如下：<br><img src="C4-5%E7%AE%97%E6%B3%95Python%E5%AE%9E%E7%8E%B0/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%8E%87.png" alt="增益率"></p>
<h3 id="4-4数据离散化"><a href="#4-4数据离散化" class="headerlink" title="4.4数据离散化"></a>4.4数据离散化</h3><p>当处理连续型取值的数据时，就不能直接计算属性的信息增益率了，这时属性的取值种类太多，生成的决策树太过庞大，导致错误率上升。这时就需要对数据进行离散化处理。<br>数据的离散化是在决策树构造中最优属性选择之前完成的。这时离散性属性取值只有两类：大于最佳分类点的那一类和小于最佳分裂点的那一类。也就是说，这时决策树的结点具体有两类：属性、属性的取值。对于属性：离散型取值属性即为属性名称，连续型取值属性为’&lt;’或’&gt;’最佳分裂值。对于属性的取值：对于离散型取值的属性来讲，属性下的那些子节点就是每个不同的离散型取值<em>（多分支形式，子节点不止一个）</em>。而对于连续性取值的属性来讲，属性下的子节点只有两个：’是’:字节点为’&lt;’或’&gt;’最佳分裂点的那一类取值<em>（二叉树形式）</em>。</p>
<h3 id="4-5决策树决策过程的思路"><a href="#4-5决策树决策过程的思路" class="headerlink" title="4.5决策树决策过程的思路"></a>4.5决策树决策过程的思路</h3><p>见图：<em>（来自网上截图）</em><br><img src="C4-5%E7%AE%97%E6%B3%95Python%E5%AE%9E%E7%8E%B0/%E5%86%B3%E7%AD%96%E6%A0%91%E6%80%9D%E8%B7%AF.png" alt="决策树思路"></p>
<h3 id="4-6-个人笔记"><a href="#4-6-个人笔记" class="headerlink" title="4.6 个人笔记"></a>4.6 个人笔记</h3><p>笔记图片如下：<em>（字丑见谅）</em><br><img src="C4-5%E7%AE%97%E6%B3%95Python%E5%AE%9E%E7%8E%B0/%E7%AC%94%E8%AE%B0.png" alt="笔记1"><br><img src="C4-5%E7%AE%97%E6%B3%95Python%E5%AE%9E%E7%8E%B0/%E7%AC%94%E8%AE%B0%E2%80%98.JPG" alt="笔记2"></p>
<h1 id="2-代码："><a href="#2-代码：" class="headerlink" title="2.代码："></a>2.代码：</h1><p><em>因算法较抽象，故注释较多，怕以后忘记 : )</em></p>
<pre><code class="python"><span class="keyword">from</span> numpy <span class="keyword">import</span> *
<span class="keyword">import</span> csv  
<span class="keyword">import</span> copy
<span class="keyword">from</span> math <span class="keyword">import</span> log
<span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt
decisionNode = dict(boxstyle=<span class="string">"sawtooth"</span> , fc=<span class="string">"0.8"</span>)
leafNode = dict(boxstyle=<span class="string">"round4"</span> , fc=<span class="string">"0.8"</span>)
arrow_args = dict(arrowstyle=<span class="string">"&lt;-"</span>)

<span class="function"><span class="keyword">def</span> <span class="title">Init_Data_Set</span><span class="params">()</span>:</span><span class="comment">#初始化数据,获得训练集和测试集   </span>
    <span class="keyword">with</span> open(<span class="string">'C:/Users/Lenovo/Desktop/hamster.csv'</span>, <span class="string">'r'</span> , encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:
        labels = []  <span class="comment">#获得属性集合</span>
        reader = csv.reader(f)
        m = <span class="number">0</span>
        Data = []
        <span class="keyword">for</span> row <span class="keyword">in</span> reader:
            Data.append(row)
            m += <span class="number">1</span>
            n = shape(row)[<span class="number">0</span>]
    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span> , m):
        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(Data[i])):
            Data[i][j] = float(Data[i][j])
    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span> , n<span class="number">-1</span>):
        labels.append(Data[<span class="number">0</span>][k])
    train_num = int(m * <span class="number">0.7</span>)
    test_num = m - train_num
    train_dataMat = [] 
    train_labelMat = []
    test_dataMat = []
    test_labelMat = []
    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span> , train_num+<span class="number">1</span>):  <span class="comment">#得到训练数据及其类别</span>
        train_dataMat.append(Data[i][<span class="number">1</span>:n])
        train_labelMat.append(Data[i][<span class="number">-1</span>])
    <span class="keyword">for</span> j <span class="keyword">in</span> range(train_num+<span class="number">1</span> , m):  <span class="comment">#得到测试数据及其类别</span>
        test_dataMat.append(Data[j][<span class="number">1</span>:n])
        test_labelMat.append(Data[j][<span class="number">-1</span>])
    <span class="keyword">return</span> train_dataMat , train_labelMat , train_num , test_dataMat , test_labelMat , test_num , labels  <span class="comment">#返回训练数据集合测试数据集的数据数组和类型数组</span>

<span class="function"><span class="keyword">def</span> <span class="title">label_property</span><span class="params">(dataSet)</span>:</span>  <span class="comment">#得到属取值性类别数组，0代表离散型取值，1代表连续型取值</span>
    set_num ,property_num = shape(dataSet)
    property_num = property_num - <span class="number">1</span>
    Label_Property = zeros(property_num)
    <span class="keyword">for</span> i <span class="keyword">in</span> range(property_num):
        values = zeros(set_num)
        <span class="keyword">for</span> j <span class="keyword">in</span> range(set_num):
            values[j] = dataSet[j][i]
        values = set(values)
        <span class="keyword">if</span> len(values) &gt; <span class="number">5</span>:
            Label_Property[i] = <span class="number">1</span>
        <span class="keyword">else</span>:
            Label_Property[i] = <span class="number">0</span>
    <span class="keyword">return</span> Label_Property


<span class="function"><span class="keyword">def</span> <span class="title">ShannonEnt</span><span class="params">(dataSet)</span>:</span><span class="comment">#返回香农熵</span>
    numEntries = len(dataSet)
    labelCounts = {}  <span class="comment">#类别数量字典，其中键为类别为名称，对应值为改类别的个数</span>
    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:  <span class="comment">#统计所有类别及其数量</span>
        currentlabel = featVec[<span class="number">-1</span>] <span class="comment">#获得该类别</span>
        <span class="keyword">if</span> currentlabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():  <span class="comment">#该类别还没加到字典里</span>
            labelCounts[currentlabel] = <span class="number">0</span>
        labelCounts[currentlabel] += <span class="number">1</span>
    shannonEnt = <span class="number">0.0</span>  <span class="comment">#香农商</span>
    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:  <span class="comment">#对每种类型求香农熵的增量</span>
        prob = float(labelCounts[key])/numEntries  <span class="comment">#计算每个类型在所有数据中的比例</span>
        shannonEnt -= prob * log(prob , <span class="number">2</span>)  <span class="comment">#香农熵对于每一项的增量公式</span>
    <span class="keyword">return</span> shannonEnt  <span class="comment">#返回香农熵</span>

<span class="function"><span class="keyword">def</span> <span class="title">Split_DataSet_discrete</span><span class="params">(dataSet , axis , value)</span>:</span>  <span class="comment">#处理离散型取值属性时返回数据中属性 axis 值为 value 的所有数据集合（去除属性 axis 之后的，相当于一个子集合）</span>
    reDataSet = []
    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:
        <span class="keyword">if</span> featVec[axis] == value:
            reducedFeatVec = featVec[:axis]
            reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])
            reDataSet.append(reducedFeatVec)
    <span class="keyword">return</span> reDataSet  <span class="comment">#返回一个除去属性 axis 值为 value 的数据集合，此集合中没有 axis 这个属性 </span>

<span class="function"><span class="keyword">def</span> <span class="title">Split_DataSet_continury</span><span class="params">(DataSet , axis , value , LorR = <span class="string">'L'</span>)</span>:</span>  <span class="comment">#处理连续型取值属性axis时，按照最佳分类点value进行数据划分，返回在分裂点左边或右边(LorR默认'L'左边）的数据集</span>
    retDataSet = []  <span class="comment">#存放连续性取值属性值在value左边或右边的数据集</span>
    featVec = []
    <span class="keyword">if</span> LorR == <span class="string">'L'</span>:  <span class="comment">#若要左边的数据</span>
        <span class="keyword">for</span> featVec <span class="keyword">in</span> DataSet:
            <span class="keyword">if</span> float(featVec[axis]) &lt; value:
                retDataSet.append(featVec)
    <span class="keyword">else</span>:  <span class="comment">#若要右边的数据</span>
        <span class="keyword">for</span> featVec <span class="keyword">in</span> DataSet:
            <span class="keyword">if</span> float(featVec[axis]) &gt; value:
                retDataSet.append(featVec)
    <span class="keyword">return</span> retDataSet  <span class="comment">#返回需要区间的数据集</span>

<span class="function"><span class="keyword">def</span> <span class="title">Find_Best_Feature</span><span class="params">(dataSet , Label_Propety)</span>:</span>  <span class="comment">#找出最好的划分属性，Label_Property：存放属性性质的数组，离散型：0，连续性：1</span>
    num_Features = len(dataSet[<span class="number">0</span>])<span class="number">-1</span>  <span class="comment">#求属性(列)个数</span>
    base_Entropy = ShannonEnt(dataSet)  <span class="comment">#整个样本的复杂度</span>
    best_InfoGain = <span class="number">0.0</span>
    best_Festure = <span class="number">-1</span>  <span class="comment">#最好的划分属性</span>
    best_Part_Value = <span class="literal">None</span>  <span class="comment">#连续型属性的最佳分裂点</span>
    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_Features):  <span class="comment">#对于每一个属性</span>
        feat_List = set([example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet])  <span class="comment">#建立一个无重复的属性取值合集，[example...]是一个列表生成式，将 dataSet 中第i列元素生成一个列表</span>
        new_Entropy = <span class="number">0.0</span>  <span class="comment">#属性 i 取值的复杂度</span>
        best_Part_Valuei = <span class="literal">None</span>
        <span class="comment">#离散型数据处理:</span>
        <span class="keyword">if</span> Label_Propety[i] == <span class="number">0</span>:  
            splitInfo = <span class="number">0.0</span>  <span class="comment">#属性 i 的复杂度</span>
            <span class="keyword">for</span> value <span class="keyword">in</span> feat_List:  <span class="comment">#计算每个属性的信息增益</span>
                sub_DataSet = Split_DataSet_discrete(dataSet , i , value)  <span class="comment">#分离出数据集中该属性值为 value 的数据，建立新集合</span>
                prob = len(sub_DataSet)/float(len(dataSet))  <span class="comment">#该属性取值为 value 的占比</span>
                new_Entropy += prob * ShannonEnt(sub_DataSet)  <span class="comment">#计算属性 i 对应的不同取值的复杂度</span>
                splitInfo -= prob * log(prob , <span class="number">2</span>)  <span class="comment">#属性 i 的复杂度</span>
            <span class="keyword">if</span> splitInfo == <span class="number">0</span>:
                <span class="keyword">continue</span>  <span class="comment">#若该属性只有一个取值，则对分类没有贡献,跳过更新阶段</span>
            infoGain = (base_Entropy - new_Entropy) / splitInfo  <span class="comment">#计算属性 i 的信息增益率</span>
            print(<span class="string">"第"</span> , i , <span class="string">"个属性"</span> , <span class="string">"的信息增益率为"</span> , infoGain)
            <span class="keyword">if</span>(infoGain &gt; best_InfoGain):  <span class="comment">#若找到新的信息增益率更大的属性，则更新最优属性</span>
                best_InfoGain = infoGain
                best_Festure = i
        <span class="comment">#连续性取值处理:</span>
        <span class="keyword">else</span>:
            Sorted_Feat_List = list(feat_List)  <span class="comment">#对特征进行排序</span>
            Sorted_Feat_List.sort()
            minEntropy = inf
            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(Sorted_Feat_List) - <span class="number">1</span>):  <span class="comment">#对于从小到大的每一个属性取值</span>
                partValue = (float(Sorted_Feat_List[j]) + float(Sorted_Feat_List[j + <span class="number">1</span>])) / <span class="number">2</span>  <span class="comment">#计算两个属性取值的中点</span>
                DataSet_Left = Split_DataSet_continury(dataSet , i , partValue , <span class="string">'L'</span>)  <span class="comment">#分出分裂点左边的数据</span>
                DataSet_Right = Split_DataSet_continury(dataSet , i , partValue , <span class="string">'R'</span>)  <span class="comment">#分出分裂点右边的数据</span>
                Prob_Left = len(DataSet_Left) / float(len(dataSet))  <span class="comment">#左边占比数据</span>
                Prob_Right = len(DataSet_Right) / float(len(dataSet))  <span class="comment">#右边数据占比</span>
                Entropy = Prob_Left * ShannonEnt(DataSet_Left) + Prob_Right * ShannonEnt(DataSet_Right)  <span class="comment">#算出本次分裂点的信息熵</span>
                <span class="keyword">if</span> Entropy &lt; minEntropy:  <span class="comment">#若找到更小的信息熵，则替换原先的</span>
                    minEntropy = Entropy  <span class="comment">#更新最小信息熵</span>
                    best_Part_Valuei = partValue  <span class="comment">#更新最好的分裂点</span>
            new_Entropy = minEntropy  <span class="comment">#得到最小信息熵</span>
            infoGain = base_Entropy - new_Entropy  <span class="comment">#计算本属性的信息增益</span>
            print(<span class="string">"第"</span> , i , <span class="string">"个属性"</span> , <span class="string">"的信息增益率为"</span> , infoGain)
            <span class="keyword">if</span> infoGain &gt; best_InfoGain:  <span class="comment">#若找到更大的信息增益</span>
                best_InfoGain = infoGain  <span class="comment">#替换最大信息增益</span>
                best_Festure = i  <span class="comment">#找到对应划分属性</span>
                best_Part_Value = best_Part_Valuei  <span class="comment">#找到连续性取值的分裂点</span>
    print(<span class="string">"最佳划分属性"</span> , best_Festure , <span class="string">"的最佳分裂点为"</span> , best_Part_Value)
    <span class="keyword">return</span> best_Festure , best_Part_Value  <span class="comment">#返回最好的划分属性</span>

<span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(class_List)</span>:</span>  <span class="comment">#找出样本中出现次数最多的类</span>
    class_Count = {}
    <span class="keyword">for</span> vote <span class="keyword">in</span> class_List:  <span class="comment">#统计各个类的出现次数</span>
        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> class_Count.keys():
            class_Count[vote] = <span class="number">0</span>
        class_Count += <span class="number">1</span>
    sorted_classCount = sorted(class_Count.items() , key = operator.itemgetter(<span class="number">1</span>) , reverse=<span class="literal">True</span>)
    <span class="keyword">return</span> sorted_classCount[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment">#返回出现次数最多的类</span>

<span class="function"><span class="keyword">def</span> <span class="title">creatTree</span><span class="params">(dataSet , labels , label_Property)</span>:</span>  
    new_labels = copy.deepcopy(labels)
    class_list = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  <span class="comment">#提取所有类别的集合，建成一个字点，键为类别，值为每一组数据</span>
    <span class="comment"># 特殊情况处理(到底层叶子结点了）：</span>
    <span class="keyword">if</span> class_list.count(class_list[<span class="number">0</span>]) == len(class_list):  <span class="comment">#若所有数据只有一个类别</span>
        <span class="keyword">return</span> class_list[<span class="number">0</span>]  <span class="comment">#则返回该类别</span>
    <span class="keyword">if</span> (len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>):  <span class="comment">#若所有特征都遍历完了，返回次数最多的类别</span>
        <span class="keyword">return</span> majorityCnt(class_list)  <span class="comment">#则返回出现次数最多的类别</span>
    <span class="comment"># 一般情况（还在内部结点）：</span>
    best_Feat , best_Part_Value = Find_Best_Feature(dataSet , label_Property)  <span class="comment">#获得当前最好的划分属性代号</span>
    <span class="keyword">if</span> best_Feat == <span class="number">-1</span>:  <span class="comment">#无法选出最优特征，返回出现次数最多的类别</span>
        <span class="keyword">return</span> majorityCnt(class_list)
    <span class="comment">#对于离散型取值属性：</span>
    <span class="keyword">if</span> label_Property[best_Feat] == <span class="number">0</span>:
        best_Feat_Label = labels[best_Feat]  <span class="comment">#获得当前最好的划分属性</span>
        myTree = {best_Feat_Label:{}}  <span class="comment">#以best_Feat_Label为根节点创建子树</span>
        labels_property_New = copy.copy(label_Property)
        <span class="keyword">del</span>(labels[best_Feat])  <span class="comment">#删除已划分结点</span>
        <span class="keyword">del</span>(labels_property_New[best_Feat])
        featValues = set([example[best_Feat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet])  <span class="comment">#创建该属性无重复的取值的集合</span>
        <span class="keyword">for</span> value <span class="keyword">in</span> featValues:  <span class="comment">#对于每一个属性，递归选择不同的属性值来构造树</span>
            sub_Labels = labels_property_New[:]
            sub_Label_Property = labels_property_New[:]
            myTree[best_Feat_Label][value] = creatTree(Split_DataSet_discrete(dataSet , best_Feat , value) , sub_Labels , sub_Label_Property)  <span class="comment">#子树的值为递归的子树</span>
    <span class="comment">#对于连续型取值数型:</span>
    <span class="keyword">else</span>:
        best_Feat_Label = str(new_labels[best_Feat]) + <span class="string">'&lt;'</span> + str(best_Part_Value)
        myTree = {best_Feat_Label:{}}
        sub_Labels = labels[:]
        sub_Label_Property = label_Property[:]
        <span class="comment">#构建左子树：</span>
        value_Left = <span class="string">'是'</span>
        myTree[best_Feat_Label][value_Left] = creatTree(Split_DataSet_continury(dataSet , best_Feat , best_Part_Value , <span class="string">'L'</span>) , sub_Labels , sub_Label_Property)
        <span class="comment">#创建右子树：</span>
        value_Right = <span class="string">'否'</span>
        myTree[best_Feat_Label][value_Right] = creatTree(Split_DataSet_continury(dataSet , best_Feat , best_Part_Value , <span class="string">'R'</span>) , sub_Labels ,  sub_Label_Property)
    <span class="keyword">return</span> myTree
    <span class="comment">#树的形式：T={属性：{值1:{T1...},值2:{T2...}}}</span>


<span class="function"><span class="keyword">def</span> <span class="title">Classifier</span><span class="params">(InputTree , Feat_Labels , Feat_Label_Properties , Test_Vec)</span>:</span>  <span class="comment">#分类器                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           </span>
    First_Str = list(InputTree.keys())[<span class="number">0</span>]  <span class="comment">#返回最外层的所有键，这里是树根的属性</span>
    First_Label = First_Str  <span class="comment">#获得属性的名字</span>
    less_Index = str(First_Str).find(<span class="string">'&lt;'</span>)  
    <span class="keyword">if</span> less_Index &gt; <span class="number">-1</span>:  <span class="comment">#若在属性名字里找到'&lt;'，则说明是一个连续型取值属性</span>
        First_Label = str(First_Str)[:less_Index]
    Second_Dict = InputTree[First_Str]  <span class="comment">#获取子树，即本层属性所有的取值及取值的子树</span>
    Feat_Index = list(Feat_Labels).index(First_Label)  <span class="comment">#获取本层属性所对应的类别编号</span>
    Class_Label = <span class="literal">None</span>
    <span class="keyword">for</span> key <span class="keyword">in</span> list(Second_Dict.keys()):  <span class="comment">#对于子树中的每个取值及子树</span>
        <span class="keyword">if</span> Test_Vec[Feat_Index] == key:  <span class="comment">#如果本层属性取值为这一个取值</span>
            <span class="comment">#离散型取值处理:</span>
            <span class="keyword">if</span> Feat_Label_Properties[Feat_Index] == <span class="number">0</span>:  
                <span class="keyword">if</span> Test_Vec[Feat_Index] == key:  <span class="comment">#进入某个分支</span>
                    <span class="keyword">if</span> isinstance(Second_Dict[key] , dict):  <span class="comment">#若还没到叶子结点</span>
                        Class_Label = Classifier(Second_Dict[key] , Feat_Labels ,  Feat_Label_Properties , Test_Vec)  <span class="comment">#在子树zhong查找</span>
                    <span class="keyword">else</span>:  <span class="comment">#否则，该结点取值即为寻样本的类型</span>
                        Class_Label = Second_Dict[key]
            <span class="comment">#连续型取值处理:</span>
            <span class="keyword">else</span>:
                part_Value = float(str(First_Str)[less_Index + <span class="number">1</span>:])  <span class="comment">#获取分裂点的值</span>
                <span class="keyword">if</span> Test_Vec[Feat_Index] &lt; part_Value:  <span class="comment">#进入左子树</span>
                    <span class="keyword">if</span> isinstance(Second_Dict[<span class="string">'是'</span>] , dict):  <span class="comment">#未到叶子结点，在子树中判断</span>
                        Class_Label = Classifier(Second_Dict[<span class="string">'是'</span>] , Feat_Labels , Feat_Label_Properties , Test_Vec)
                    <span class="keyword">else</span>:
                        Class_Label = Second_Dict[<span class="string">'是'</span>]
                <span class="keyword">else</span>:
                    <span class="keyword">if</span> isinstance(Second_Dict[<span class="string">'否'</span>] , dict):  <span class="comment">#未到叶子结点，在子树中判断</span>
                        Class_Label = Classifier(Second_Dict[<span class="string">'否'</span>] , Feat_Labels , Feat_Label_Properties , Test_Vec)
                    <span class="keyword">else</span>:
                        Class_Label = Second_Dict[<span class="string">'否'</span>]
    <span class="keyword">return</span> Class_Label  <span class="comment">#返回分类类型</span>

<span class="function"><span class="keyword">def</span> <span class="title">ColicTest</span><span class="params">(inputTree , test_dataMat , test_labelMat , labels, test_num , Feat_Label_Properties)</span>:</span><span class="comment">#给出算法的错误率</span>
    errorCount = <span class="number">0</span>
    numTestVec = test_num
    <span class="keyword">for</span> i <span class="keyword">in</span> range(test_num<span class="number">-1</span>):
        <span class="keyword">if</span> Classifier(inputTree , labels , Feat_Label_Properties , test_dataMat) != test_labelMat[i]:
            errorCount += <span class="number">1</span>
    errorRate = (float(errorCount) / numTestVec)
    print(<span class="string">"The error rate of this test is :"</span> , errorRate)
    <span class="keyword">return</span> errorRate<span class="comment">#通过给出的训练集和测试集计算出最佳权重，并给出错误率</span>


<span class="function"><span class="keyword">def</span> <span class="title">multiTest</span><span class="params">(Input_Tree , test_dataMat , test_labelMat , labels , test_num , Feat_Label_Properties)</span>:</span>  <span class="comment">#计算平均误率</span>
    numTests = <span class="number">10</span>
    errorSum = <span class="number">0.0</span>
    <span class="keyword">for</span> k <span class="keyword">in</span> range(numTests):
        print(k)
        errorSum += ColicTest(Input_Tree , test_dataMat , test_labelMat , labels , test_num , Feat_Label_Properties)
    print(<span class="string">"After "</span> , numTests , <span class="string">"iterations the average error rate is:"</span> , errorSum/float(numTests))


train_data , train_label , train_num , test_data , test_label , test_num , labels = Init_Data_Set() <span class="comment">#获得训练集和测试集</span>
label_Property = label_property(train_data)
my_Tree = creatTree(train_data , labels , label_Property)  <span class="comment">#根据训练集创建决策树</span>
multiTest(my_Tree , test_data , test_label , labels , test_num , label_Property)  <span class="comment">#利用决策树进行分类，并计算错误率,并输出</span></code></pre>
