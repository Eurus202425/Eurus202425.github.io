<!DOCTYPE html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>
<meta name="theme-color" content="#222">









<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />
















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />







<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.4" rel="stylesheet" type="text/css" />


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=5.1.4">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=5.1.4">


  <link rel="mask-icon" href="/images/logo.svg?v=5.1.4" color="#222">





  <meta name="keywords" content="Hexo, NexT" />










<meta property="og:type" content="website">
<meta property="og:title" content="Eurus的博客">
<meta property="og:url" content="http:&#x2F;&#x2F;yoursite.com&#x2F;index.html">
<meta property="og:site_name" content="Eurus的博客">
<meta property="og:locale" content="zh-Hans">
<meta name="twitter:card" content="summary">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '5.1.4',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false},
    fancybox: true,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/"/>





  <title>Eurus的博客</title>
  








</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Eurus的博客</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle">Welcome !</p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives/%20" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2019/10/27/C4-5%E7%AE%97%E6%B3%95Python%E5%AE%9E%E7%8E%B0/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Eurus">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Eurus的博客">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                <a class="post-title-link" href="/2019/10/27/C4-5%E7%AE%97%E6%B3%95Python%E5%AE%9E%E7%8E%B0/" itemprop="url">机器学习笔记（1）—————— C4.5算法，Python实现(陆续更新)</a></h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2019-10-27T13:28:54+08:00">
                2019-10-27
              </time>
            

            

            
          </span>

          

          
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>决策树类型算法主要有ID3、C4.5和CART算法。这篇博客主要研究C4.5算法的原理及实现过程。<em>(纯属小白、写的不完善 、陆续更新优化 : )，增加画图、剪枝等功能)</em></p>
<h1 id="1-原理："><a href="#1-原理：" class="headerlink" title="1.原理："></a>1.原理：</h1><h2 id="1）思想："><a href="#1）思想：" class="headerlink" title="1）思想："></a>1）思想：</h2><p>总体思路为：通过训练集计算属性的信息熵、信息增益和信息增益率来判定当前最优的划分属性（即信息增益最大的属性），通过选出的最优属性依次递归构造二叉树（通过字典存储）。再利用构造好的二叉树对测试数据进行分类。</p>
<h2 id="2）类别：监督学习"><a href="#2）类别：监督学习" class="headerlink" title="2）类别：监督学习"></a>2）类别：监督学习</h2><h2 id="3）优点："><a href="#3）优点：" class="headerlink" title="3）优点："></a>3）优点：</h2><p>3.1 能够处理连续型取值的属性<br>3.2 通过信息增益率而不是信息增益来选择最优划分属性，避免选择偏向分支更多的属性<br>3.3 可处理缺失值<br>3.4 可对树进行预剪枝，避免造成过拟合</p>
<h2 id="4）基本概念和数学公式："><a href="#4）基本概念和数学公式：" class="headerlink" title="4）基本概念和数学公式："></a>4）基本概念和数学公式：</h2><h3 id="4-1-信息熵"><a href="#4-1-信息熵" class="headerlink" title="4.1 信息熵"></a>4.1 信息熵</h3><p>“信息熵”是度量样本集合纯度最常用的一种指标。（个人理解是，信息熵代表数据的混乱程度。熵增，复杂度上升；熵减，复杂度下降，若一个分类使得信息熵减小，则混乱程度下降，说明有助于我们分类）公式如下：<br><img src="https://github.com/Eurus202425/Eurus202425.github.io/blob/master/2019/10/27/C4-5%E7%AE%97%E6%B3%95Python%E5%AE%9E%E7%8E%B0/%E4%BF%A1%E6%81%AF%E7%86%B5.png" alt="信息熵"></p>
<h3 id="4-2-信息增益"><a href="#4-2-信息增益" class="headerlink" title="4.2 信息增益"></a>4.2 信息增益</h3><p>“信息增益”是分类后的信息熵相对于原信息熵的减少量，也就是说信息增益越大，信息熵减小得越多，分类越有效。公式如下：<br><img src="https://github.com/Eurus202425/Eurus202425.github.io/blob/master/2019/10/27/C4-5%E7%AE%97%E6%B3%95Python%E5%AE%9E%E7%8E%B0/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A.png" alt="信息增益"></p>
<h3 id="4-3-增益率"><a href="#4-3-增益率" class="headerlink" title="4.3 增益率"></a>4.3 增益率</h3><p>原本信息增益就可以解决分类问题，那为什么要提出信息增益率之一概念呢？原因如下：信息增益有一个缺点，当一个属性的取值多而松散时*（比如 ID 这个属性，取值不重复且无规律，对分类并无贡献，但在计算信息增益时，信息增益会偏大，对于属性的选择会造成误导。但是若使用信息增益率，则能很好的解决这一问题。图如下：<br><img src="https://github.com/Eurus202425/Eurus202425.github.io/blob/master/2019/10/27/C4-5%E7%AE%97%E6%B3%95Python%E5%AE%9E%E7%8E%B0/%E4%BF%A1%E6%81%AF%E5%A2%9E%E7%9B%8A%E7%8E%87.png" alt="增益率"></p>
<h3 id="4-4数据离散化"><a href="#4-4数据离散化" class="headerlink" title="4.4数据离散化"></a>4.4数据离散化</h3><p>当处理连续型取值的数据时，就不能直接计算属性的信息增益率了，这时属性的取值种类太多，生成的决策树太过庞大，导致错误率上升。这时就需要对数据进行离散化处理。<br>数据的离散化是在决策树构造中最优属性选择之前完成的。这时离散性属性取值只有两类：大于最佳分类点的那一类和小于最佳分裂点的那一类。也就是说，这时决策树的结点具体有两类：属性、属性的取值。对于属性：离散型取值属性即为属性名称，连续型取值属性为’&lt;’或’&gt;’最佳分裂值。对于属性的取值：对于离散型取值的属性来讲，属性下的那些子节点就是每个不同的离散型取值<em>（多分支形式，子节点不止一个）</em>。而对于连续性取值的属性来讲，属性下的子节点只有两个：’是’:字节点为’&lt;’或’&gt;’最佳分裂点的那一类取值<em>（二叉树形式）</em>。</p>
<h3 id="4-5决策树决策过程的思路"><a href="#4-5决策树决策过程的思路" class="headerlink" title="4.5决策树决策过程的思路"></a>4.5决策树决策过程的思路</h3><p>见图：<em>（来自网上截图）</em><br><img src="https://github.com/Eurus202425/Eurus202425.github.io/blob/master/2019/10/27/C4-5%E7%AE%97%E6%B3%95Python%E5%AE%9E%E7%8E%B0/%E5%86%B3%E7%AD%96%E6%A0%91%E6%80%9D%E8%B7%AF.png" alt="决策树思路"></p>
<h3 id="4-6-个人笔记"><a href="#4-6-个人笔记" class="headerlink" title="4.6 个人笔记"></a>4.6 个人笔记</h3><p>笔记图片如下：<em>（字丑见谅）</em><br><img src="https://github.com/Eurus202425/Eurus202425.github.io/blob/master/2019/10/27/C4-5%E7%AE%97%E6%B3%95Python%E5%AE%9E%E7%8E%B0/%E7%AC%94%E8%AE%B0.png" alt="笔记1"><br><img src="https://github.com/Eurus202425/Eurus202425.github.io/blob/master/2019/10/27/C4-5%E7%AE%97%E6%B3%95Python%E5%AE%9E%E7%8E%B0/%E7%AC%94%E8%AE%B0%E2%80%98.JPG" alt="笔记2"></p>
<h1 id="2-代码："><a href="#2-代码：" class="headerlink" title="2.代码："></a>2.代码：</h1><p><em>因算法较抽象，故注释较多，怕以后忘记 : )</em></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> csv  </span><br><span class="line"><span class="keyword">import</span> copy</span><br><span class="line"><span class="keyword">from</span> math <span class="keyword">import</span> log</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">decisionNode = dict(boxstyle=<span class="string">"sawtooth"</span> , fc=<span class="string">"0.8"</span>)</span><br><span class="line">leafNode = dict(boxstyle=<span class="string">"round4"</span> , fc=<span class="string">"0.8"</span>)</span><br><span class="line">arrow_args = dict(arrowstyle=<span class="string">"&lt;-"</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Init_Data_Set</span><span class="params">()</span>:</span><span class="comment">#初始化数据,获得训练集和测试集   </span></span><br><span class="line">    <span class="keyword">with</span> open(<span class="string">'C:/Users/Lenovo/Desktop/hamster.csv'</span>, <span class="string">'r'</span> , encoding=<span class="string">'utf-8'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        labels = []  <span class="comment">#获得属性集合</span></span><br><span class="line">        reader = csv.reader(f)</span><br><span class="line">        m = <span class="number">0</span></span><br><span class="line">        Data = []</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> reader:</span><br><span class="line">            Data.append(row)</span><br><span class="line">            m += <span class="number">1</span></span><br><span class="line">            n = shape(row)[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span> , m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(Data[i])):</span><br><span class="line">            Data[i][j] = float(Data[i][j])</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(<span class="number">1</span> , n<span class="number">-1</span>):</span><br><span class="line">        labels.append(Data[<span class="number">0</span>][k])</span><br><span class="line">    train_num = int(m * <span class="number">0.7</span>)</span><br><span class="line">    test_num = m - train_num</span><br><span class="line">    train_dataMat = [] </span><br><span class="line">    train_labelMat = []</span><br><span class="line">    test_dataMat = []</span><br><span class="line">    test_labelMat = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span> , train_num+<span class="number">1</span>):  <span class="comment">#得到训练数据及其类别</span></span><br><span class="line">        train_dataMat.append(Data[i][<span class="number">1</span>:n])</span><br><span class="line">        train_labelMat.append(Data[i][<span class="number">-1</span>])</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> range(train_num+<span class="number">1</span> , m):  <span class="comment">#得到测试数据及其类别</span></span><br><span class="line">        test_dataMat.append(Data[j][<span class="number">1</span>:n])</span><br><span class="line">        test_labelMat.append(Data[j][<span class="number">-1</span>])</span><br><span class="line">    <span class="keyword">return</span> train_dataMat , train_labelMat , train_num , test_dataMat , test_labelMat , test_num , labels  <span class="comment">#返回训练数据集合测试数据集的数据数组和类型数组</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">label_property</span><span class="params">(dataSet)</span>:</span>  <span class="comment">#得到属取值性类别数组，0代表离散型取值，1代表连续型取值</span></span><br><span class="line">    set_num ,property_num = shape(dataSet)</span><br><span class="line">    property_num = property_num - <span class="number">1</span></span><br><span class="line">    Label_Property = zeros(property_num)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(property_num):</span><br><span class="line">        values = zeros(set_num)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(set_num):</span><br><span class="line">            values[j] = dataSet[j][i]</span><br><span class="line">        values = set(values)</span><br><span class="line">        <span class="keyword">if</span> len(values) &gt; <span class="number">5</span>:</span><br><span class="line">            Label_Property[i] = <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Label_Property[i] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">return</span> Label_Property</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ShannonEnt</span><span class="params">(dataSet)</span>:</span><span class="comment">#返回香农熵</span></span><br><span class="line">    numEntries = len(dataSet)</span><br><span class="line">    labelCounts = &#123;&#125;  <span class="comment">#类别数量字典，其中键为类别为名称，对应值为改类别的个数</span></span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:  <span class="comment">#统计所有类别及其数量</span></span><br><span class="line">        currentlabel = featVec[<span class="number">-1</span>] <span class="comment">#获得该类别</span></span><br><span class="line">        <span class="keyword">if</span> currentlabel <span class="keyword">not</span> <span class="keyword">in</span> labelCounts.keys():  <span class="comment">#该类别还没加到字典里</span></span><br><span class="line">            labelCounts[currentlabel] = <span class="number">0</span></span><br><span class="line">        labelCounts[currentlabel] += <span class="number">1</span></span><br><span class="line">    shannonEnt = <span class="number">0.0</span>  <span class="comment">#香农商</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> labelCounts:  <span class="comment">#对每种类型求香农熵的增量</span></span><br><span class="line">        prob = float(labelCounts[key])/numEntries  <span class="comment">#计算每个类型在所有数据中的比例</span></span><br><span class="line">        shannonEnt -= prob * log(prob , <span class="number">2</span>)  <span class="comment">#香农熵对于每一项的增量公式</span></span><br><span class="line">    <span class="keyword">return</span> shannonEnt  <span class="comment">#返回香农熵</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Split_DataSet_discrete</span><span class="params">(dataSet , axis , value)</span>:</span>  <span class="comment">#处理离散型取值属性时返回数据中属性 axis 值为 value 的所有数据集合（去除属性 axis 之后的，相当于一个子集合）</span></span><br><span class="line">    reDataSet = []</span><br><span class="line">    <span class="keyword">for</span> featVec <span class="keyword">in</span> dataSet:</span><br><span class="line">        <span class="keyword">if</span> featVec[axis] == value:</span><br><span class="line">            reducedFeatVec = featVec[:axis]</span><br><span class="line">            reducedFeatVec.extend(featVec[axis+<span class="number">1</span>:])</span><br><span class="line">            reDataSet.append(reducedFeatVec)</span><br><span class="line">    <span class="keyword">return</span> reDataSet  <span class="comment">#返回一个除去属性 axis 值为 value 的数据集合，此集合中没有 axis 这个属性 </span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Split_DataSet_continury</span><span class="params">(DataSet , axis , value , LorR = <span class="string">'L'</span>)</span>:</span>  <span class="comment">#处理连续型取值属性axis时，按照最佳分类点value进行数据划分，返回在分裂点左边或右边(LorR默认'L'左边）的数据集</span></span><br><span class="line">    retDataSet = []  <span class="comment">#存放连续性取值属性值在value左边或右边的数据集</span></span><br><span class="line">    featVec = []</span><br><span class="line">    <span class="keyword">if</span> LorR == <span class="string">'L'</span>:  <span class="comment">#若要左边的数据</span></span><br><span class="line">        <span class="keyword">for</span> featVec <span class="keyword">in</span> DataSet:</span><br><span class="line">            <span class="keyword">if</span> float(featVec[axis]) &lt; value:</span><br><span class="line">                retDataSet.append(featVec)</span><br><span class="line">    <span class="keyword">else</span>:  <span class="comment">#若要右边的数据</span></span><br><span class="line">        <span class="keyword">for</span> featVec <span class="keyword">in</span> DataSet:</span><br><span class="line">            <span class="keyword">if</span> float(featVec[axis]) &gt; value:</span><br><span class="line">                retDataSet.append(featVec)</span><br><span class="line">    <span class="keyword">return</span> retDataSet  <span class="comment">#返回需要区间的数据集</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Find_Best_Feature</span><span class="params">(dataSet , Label_Propety)</span>:</span>  <span class="comment">#找出最好的划分属性，Label_Property：存放属性性质的数组，离散型：0，连续性：1</span></span><br><span class="line">    num_Features = len(dataSet[<span class="number">0</span>])<span class="number">-1</span>  <span class="comment">#求属性(列)个数</span></span><br><span class="line">    base_Entropy = ShannonEnt(dataSet)  <span class="comment">#整个样本的复杂度</span></span><br><span class="line">    best_InfoGain = <span class="number">0.0</span></span><br><span class="line">    best_Festure = <span class="number">-1</span>  <span class="comment">#最好的划分属性</span></span><br><span class="line">    best_Part_Value = <span class="literal">None</span>  <span class="comment">#连续型属性的最佳分裂点</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_Features):  <span class="comment">#对于每一个属性</span></span><br><span class="line">        feat_List = set([example[i] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet])  <span class="comment">#建立一个无重复的属性取值合集，[example...]是一个列表生成式，将 dataSet 中第i列元素生成一个列表</span></span><br><span class="line">        new_Entropy = <span class="number">0.0</span>  <span class="comment">#属性 i 取值的复杂度</span></span><br><span class="line">        best_Part_Valuei = <span class="literal">None</span></span><br><span class="line">        <span class="comment">#离散型数据处理:</span></span><br><span class="line">        <span class="keyword">if</span> Label_Propety[i] == <span class="number">0</span>:  </span><br><span class="line">            splitInfo = <span class="number">0.0</span>  <span class="comment">#属性 i 的复杂度</span></span><br><span class="line">            <span class="keyword">for</span> value <span class="keyword">in</span> feat_List:  <span class="comment">#计算每个属性的信息增益</span></span><br><span class="line">                sub_DataSet = Split_DataSet_discrete(dataSet , i , value)  <span class="comment">#分离出数据集中该属性值为 value 的数据，建立新集合</span></span><br><span class="line">                prob = len(sub_DataSet)/float(len(dataSet))  <span class="comment">#该属性取值为 value 的占比</span></span><br><span class="line">                new_Entropy += prob * ShannonEnt(sub_DataSet)  <span class="comment">#计算属性 i 对应的不同取值的复杂度</span></span><br><span class="line">                splitInfo -= prob * log(prob , <span class="number">2</span>)  <span class="comment">#属性 i 的复杂度</span></span><br><span class="line">            <span class="keyword">if</span> splitInfo == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span>  <span class="comment">#若该属性只有一个取值，则对分类没有贡献,跳过更新阶段</span></span><br><span class="line">            infoGain = (base_Entropy - new_Entropy) / splitInfo  <span class="comment">#计算属性 i 的信息增益率</span></span><br><span class="line">            print(<span class="string">"第"</span> , i , <span class="string">"个属性"</span> , <span class="string">"的信息增益率为"</span> , infoGain)</span><br><span class="line">            <span class="keyword">if</span>(infoGain &gt; best_InfoGain):  <span class="comment">#若找到新的信息增益率更大的属性，则更新最优属性</span></span><br><span class="line">                best_InfoGain = infoGain</span><br><span class="line">                best_Festure = i</span><br><span class="line">        <span class="comment">#连续性取值处理:</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            Sorted_Feat_List = list(feat_List)  <span class="comment">#对特征进行排序</span></span><br><span class="line">            Sorted_Feat_List.sort()</span><br><span class="line">            minEntropy = inf</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(Sorted_Feat_List) - <span class="number">1</span>):  <span class="comment">#对于从小到大的每一个属性取值</span></span><br><span class="line">                partValue = (float(Sorted_Feat_List[j]) + float(Sorted_Feat_List[j + <span class="number">1</span>])) / <span class="number">2</span>  <span class="comment">#计算两个属性取值的中点</span></span><br><span class="line">                DataSet_Left = Split_DataSet_continury(dataSet , i , partValue , <span class="string">'L'</span>)  <span class="comment">#分出分裂点左边的数据</span></span><br><span class="line">                DataSet_Right = Split_DataSet_continury(dataSet , i , partValue , <span class="string">'R'</span>)  <span class="comment">#分出分裂点右边的数据</span></span><br><span class="line">                Prob_Left = len(DataSet_Left) / float(len(dataSet))  <span class="comment">#左边占比数据</span></span><br><span class="line">                Prob_Right = len(DataSet_Right) / float(len(dataSet))  <span class="comment">#右边数据占比</span></span><br><span class="line">                Entropy = Prob_Left * ShannonEnt(DataSet_Left) + Prob_Right * ShannonEnt(DataSet_Right)  <span class="comment">#算出本次分裂点的信息熵</span></span><br><span class="line">                <span class="keyword">if</span> Entropy &lt; minEntropy:  <span class="comment">#若找到更小的信息熵，则替换原先的</span></span><br><span class="line">                    minEntropy = Entropy  <span class="comment">#更新最小信息熵</span></span><br><span class="line">                    best_Part_Valuei = partValue  <span class="comment">#更新最好的分裂点</span></span><br><span class="line">            new_Entropy = minEntropy  <span class="comment">#得到最小信息熵</span></span><br><span class="line">            infoGain = base_Entropy - new_Entropy  <span class="comment">#计算本属性的信息增益</span></span><br><span class="line">            print(<span class="string">"第"</span> , i , <span class="string">"个属性"</span> , <span class="string">"的信息增益率为"</span> , infoGain)</span><br><span class="line">            <span class="keyword">if</span> infoGain &gt; best_InfoGain:  <span class="comment">#若找到更大的信息增益</span></span><br><span class="line">                best_InfoGain = infoGain  <span class="comment">#替换最大信息增益</span></span><br><span class="line">                best_Festure = i  <span class="comment">#找到对应划分属性</span></span><br><span class="line">                best_Part_Value = best_Part_Valuei  <span class="comment">#找到连续性取值的分裂点</span></span><br><span class="line">    print(<span class="string">"最佳划分属性"</span> , best_Festure , <span class="string">"的最佳分裂点为"</span> , best_Part_Value)</span><br><span class="line">    <span class="keyword">return</span> best_Festure , best_Part_Value  <span class="comment">#返回最好的划分属性</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">majorityCnt</span><span class="params">(class_List)</span>:</span>  <span class="comment">#找出样本中出现次数最多的类</span></span><br><span class="line">    class_Count = &#123;&#125;</span><br><span class="line">    <span class="keyword">for</span> vote <span class="keyword">in</span> class_List:  <span class="comment">#统计各个类的出现次数</span></span><br><span class="line">        <span class="keyword">if</span> vote <span class="keyword">not</span> <span class="keyword">in</span> class_Count.keys():</span><br><span class="line">            class_Count[vote] = <span class="number">0</span></span><br><span class="line">        class_Count += <span class="number">1</span></span><br><span class="line">    sorted_classCount = sorted(class_Count.items() , key = operator.itemgetter(<span class="number">1</span>) , reverse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">return</span> sorted_classCount[<span class="number">0</span>][<span class="number">0</span>]  <span class="comment">#返回出现次数最多的类</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">creatTree</span><span class="params">(dataSet , labels , label_Property)</span>:</span>  </span><br><span class="line">    new_labels = copy.deepcopy(labels)</span><br><span class="line">    class_list = [example[<span class="number">-1</span>] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet]  <span class="comment">#提取所有类别的集合，建成一个字点，键为类别，值为每一组数据</span></span><br><span class="line">    <span class="comment"># 特殊情况处理(到底层叶子结点了）：</span></span><br><span class="line">    <span class="keyword">if</span> class_list.count(class_list[<span class="number">0</span>]) == len(class_list):  <span class="comment">#若所有数据只有一个类别</span></span><br><span class="line">        <span class="keyword">return</span> class_list[<span class="number">0</span>]  <span class="comment">#则返回该类别</span></span><br><span class="line">    <span class="keyword">if</span> (len(dataSet[<span class="number">0</span>]) == <span class="number">1</span>):  <span class="comment">#若所有特征都遍历完了，返回次数最多的类别</span></span><br><span class="line">        <span class="keyword">return</span> majorityCnt(class_list)  <span class="comment">#则返回出现次数最多的类别</span></span><br><span class="line">    <span class="comment"># 一般情况（还在内部结点）：</span></span><br><span class="line">    best_Feat , best_Part_Value = Find_Best_Feature(dataSet , label_Property)  <span class="comment">#获得当前最好的划分属性代号</span></span><br><span class="line">    <span class="keyword">if</span> best_Feat == <span class="number">-1</span>:  <span class="comment">#无法选出最优特征，返回出现次数最多的类别</span></span><br><span class="line">        <span class="keyword">return</span> majorityCnt(class_list)</span><br><span class="line">    <span class="comment">#对于离散型取值属性：</span></span><br><span class="line">    <span class="keyword">if</span> label_Property[best_Feat] == <span class="number">0</span>:</span><br><span class="line">        best_Feat_Label = labels[best_Feat]  <span class="comment">#获得当前最好的划分属性</span></span><br><span class="line">        myTree = &#123;best_Feat_Label:&#123;&#125;&#125;  <span class="comment">#以best_Feat_Label为根节点创建子树</span></span><br><span class="line">        labels_property_New = copy.copy(label_Property)</span><br><span class="line">        <span class="keyword">del</span>(labels[best_Feat])  <span class="comment">#删除已划分结点</span></span><br><span class="line">        <span class="keyword">del</span>(labels_property_New[best_Feat])</span><br><span class="line">        featValues = set([example[best_Feat] <span class="keyword">for</span> example <span class="keyword">in</span> dataSet])  <span class="comment">#创建该属性无重复的取值的集合</span></span><br><span class="line">        <span class="keyword">for</span> value <span class="keyword">in</span> featValues:  <span class="comment">#对于每一个属性，递归选择不同的属性值来构造树</span></span><br><span class="line">            sub_Labels = labels_property_New[:]</span><br><span class="line">            sub_Label_Property = labels_property_New[:]</span><br><span class="line">            myTree[best_Feat_Label][value] = creatTree(Split_DataSet_discrete(dataSet , best_Feat , value) , sub_Labels , sub_Label_Property)  <span class="comment">#子树的值为递归的子树</span></span><br><span class="line">    <span class="comment">#对于连续型取值数型:</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        best_Feat_Label = str(new_labels[best_Feat]) + <span class="string">'&lt;'</span> + str(best_Part_Value)</span><br><span class="line">        myTree = &#123;best_Feat_Label:&#123;&#125;&#125;</span><br><span class="line">        sub_Labels = labels[:]</span><br><span class="line">        sub_Label_Property = label_Property[:]</span><br><span class="line">        <span class="comment">#构建左子树：</span></span><br><span class="line">        value_Left = <span class="string">'是'</span></span><br><span class="line">        myTree[best_Feat_Label][value_Left] = creatTree(Split_DataSet_continury(dataSet , best_Feat , best_Part_Value , <span class="string">'L'</span>) , sub_Labels , sub_Label_Property)</span><br><span class="line">        <span class="comment">#创建右子树：</span></span><br><span class="line">        value_Right = <span class="string">'否'</span></span><br><span class="line">        myTree[best_Feat_Label][value_Right] = creatTree(Split_DataSet_continury(dataSet , best_Feat , best_Part_Value , <span class="string">'R'</span>) , sub_Labels ,  sub_Label_Property)</span><br><span class="line">    <span class="keyword">return</span> myTree</span><br><span class="line">    <span class="comment">#树的形式：T=&#123;属性：&#123;值1:&#123;T1...&#125;,值2:&#123;T2...&#125;&#125;&#125;</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Classifier</span><span class="params">(InputTree , Feat_Labels , Feat_Label_Properties , Test_Vec)</span>:</span>  <span class="comment">#分类器                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           </span></span><br><span class="line">    First_Str = list(InputTree.keys())[<span class="number">0</span>]  <span class="comment">#返回最外层的所有键，这里是树根的属性</span></span><br><span class="line">    First_Label = First_Str  <span class="comment">#获得属性的名字</span></span><br><span class="line">    less_Index = str(First_Str).find(<span class="string">'&lt;'</span>)  </span><br><span class="line">    <span class="keyword">if</span> less_Index &gt; <span class="number">-1</span>:  <span class="comment">#若在属性名字里找到'&lt;'，则说明是一个连续型取值属性</span></span><br><span class="line">        First_Label = str(First_Str)[:less_Index]</span><br><span class="line">    Second_Dict = InputTree[First_Str]  <span class="comment">#获取子树，即本层属性所有的取值及取值的子树</span></span><br><span class="line">    Feat_Index = list(Feat_Labels).index(First_Label)  <span class="comment">#获取本层属性所对应的类别编号</span></span><br><span class="line">    Class_Label = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> key <span class="keyword">in</span> list(Second_Dict.keys()):  <span class="comment">#对于子树中的每个取值及子树</span></span><br><span class="line">        <span class="keyword">if</span> Test_Vec[Feat_Index] == key:  <span class="comment">#如果本层属性取值为这一个取值</span></span><br><span class="line">            <span class="comment">#离散型取值处理:</span></span><br><span class="line">            <span class="keyword">if</span> Feat_Label_Properties[Feat_Index] == <span class="number">0</span>:  </span><br><span class="line">                <span class="keyword">if</span> Test_Vec[Feat_Index] == key:  <span class="comment">#进入某个分支</span></span><br><span class="line">                    <span class="keyword">if</span> isinstance(Second_Dict[key] , dict):  <span class="comment">#若还没到叶子结点</span></span><br><span class="line">                        Class_Label = Classifier(Second_Dict[key] , Feat_Labels ,  Feat_Label_Properties , Test_Vec)  <span class="comment">#在子树zhong查找</span></span><br><span class="line">                    <span class="keyword">else</span>:  <span class="comment">#否则，该结点取值即为寻样本的类型</span></span><br><span class="line">                        Class_Label = Second_Dict[key]</span><br><span class="line">            <span class="comment">#连续型取值处理:</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                part_Value = float(str(First_Str)[less_Index + <span class="number">1</span>:])  <span class="comment">#获取分裂点的值</span></span><br><span class="line">                <span class="keyword">if</span> Test_Vec[Feat_Index] &lt; part_Value:  <span class="comment">#进入左子树</span></span><br><span class="line">                    <span class="keyword">if</span> isinstance(Second_Dict[<span class="string">'是'</span>] , dict):  <span class="comment">#未到叶子结点，在子树中判断</span></span><br><span class="line">                        Class_Label = Classifier(Second_Dict[<span class="string">'是'</span>] , Feat_Labels , Feat_Label_Properties , Test_Vec)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        Class_Label = Second_Dict[<span class="string">'是'</span>]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    <span class="keyword">if</span> isinstance(Second_Dict[<span class="string">'否'</span>] , dict):  <span class="comment">#未到叶子结点，在子树中判断</span></span><br><span class="line">                        Class_Label = Classifier(Second_Dict[<span class="string">'否'</span>] , Feat_Labels , Feat_Label_Properties , Test_Vec)</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                        Class_Label = Second_Dict[<span class="string">'否'</span>]</span><br><span class="line">    <span class="keyword">return</span> Class_Label  <span class="comment">#返回分类类型</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ColicTest</span><span class="params">(inputTree , test_dataMat , test_labelMat , labels, test_num , Feat_Label_Properties)</span>:</span><span class="comment">#给出算法的错误率</span></span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    numTestVec = test_num</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(test_num<span class="number">-1</span>):</span><br><span class="line">        <span class="keyword">if</span> Classifier(inputTree , labels , Feat_Label_Properties , test_dataMat) != test_labelMat[i]:</span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">    errorRate = (float(errorCount) / numTestVec)</span><br><span class="line">    print(<span class="string">"The error rate of this test is :"</span> , errorRate)</span><br><span class="line">    <span class="keyword">return</span> errorRate<span class="comment">#通过给出的训练集和测试集计算出最佳权重，并给出错误率</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">multiTest</span><span class="params">(Input_Tree , test_dataMat , test_labelMat , labels , test_num , Feat_Label_Properties)</span>:</span>  <span class="comment">#计算平均误率</span></span><br><span class="line">    numTests = <span class="number">10</span></span><br><span class="line">    errorSum = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(numTests):</span><br><span class="line">        print(k)</span><br><span class="line">        errorSum += ColicTest(Input_Tree , test_dataMat , test_labelMat , labels , test_num , Feat_Label_Properties)</span><br><span class="line">    print(<span class="string">"After "</span> , numTests , <span class="string">"iterations the average error rate is:"</span> , errorSum/float(numTests))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_data , train_label , train_num , test_data , test_label , test_num , labels = Init_Data_Set() <span class="comment">#获得训练集和测试集</span></span><br><span class="line">label_Property = label_property(train_data)</span><br><span class="line">my_Tree = creatTree(train_data , labels , label_Property)  <span class="comment">#根据训练集创建决策树</span></span><br><span class="line">multiTest(my_Tree , test_data , test_label , labels , test_num , label_Property)  <span class="comment">#利用决策树进行分类，并计算错误率,并输出</span></span><br></pre></td></tr></table></figure>
          
        
      
    </div>
    
    
    

    

    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  


          </div>
          


          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    
    <div class="sidebar-inner">

      

      

      <section class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">Eurus</p>
              <p class="site-description motion-element" itemprop="description"></p>
          </div>

          <nav class="site-state motion-element">

            
              <div class="site-state-item site-state-posts">
              
                <a href="/archives/%20%7C%7C%20archive">
              
                  <span class="site-state-item-count">1</span>
                  <span class="site-state-item-name">日志</span>
                </a>
              </div>
            

            

            

          </nav>

          

          

          
          

          
          

          

        </div>
      </section>

      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Eurus</span>

  
</div>


  <div class="powered-by">由 <a class="theme-link" target="_blank" href="https://hexo.io">Hexo</a> 强力驱动</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">主题 &mdash; <a class="theme-link" target="_blank" href="https://github.com/iissnan/hexo-theme-next">NexT.Pisces</a> v5.1.4</div>




        







        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  












  
  
    <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>
  

  
  
    <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>
  

  
  
    <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  

  
  
    <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>
  


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.4"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.4"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.4"></script>



  

  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.4"></script>



  


  




	





  





  












  





  

  

  

  
  

  

  

  

</body>
</html>
